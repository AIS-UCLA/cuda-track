
Building upon the first part of the assignment, we now turn our attention to matrix multiplication in half-precision. Instead of the input matrices being FP32, we now perform computation on FP16 matrices. To take advantage of the smaller size of FP16 values, we utilize tensor cores in the SM, which are designed to take advantage of the faster FP16 computation. There are 2 CUDA APIs to program the tensor cores in a RTX 3090, `wmma` and `mma`. In this assignment, we will be utilizing the `mma` PTX instructions. Finish the TODO PTX instructions in `ptx.h` and the TODO lines in `hgemm.cu`. Once done, run `python3 test_hgemm.py` to make sure each kernel passes the sanity checks. Then, profile them with `ncu --set full --export reports/hgemm_all python3 test_hgemm.py` and answer the following questions.
